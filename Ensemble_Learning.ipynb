{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer: Ensemble Learning in machine learning is a technique where multiple models (often called “weak learners”) are combined to create a stronger, more accurate model.\n",
        "\n",
        " Key idea: Instead of relying on a single model, ensemble methods aggregate predictions from several models to reduce errors, improve generalization, and handle variance/bias better.\n",
        "\n",
        "Examples of ensemble methods:\n",
        "\n",
        "Bagging (e.g., Random Forest) → reduces variance by training models on random subsets of data.\n",
        "\n",
        "Boosting (e.g., XGBoost, AdaBoost) → reduces bias by sequentially improving weak models.\n",
        "\n",
        "Stacking → combines different models through a meta-model."
      ],
      "metadata": {
        "id": "TRQ8GRe88IVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer: Bagging vs Boosting:\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):**\n",
        "\n",
        "Trains multiple models in parallel on random subsets of data.\n",
        "\n",
        "Focuses on reducing variance.\n",
        "\n",
        "Example: Random Forest.\n",
        "\n",
        "**Boosting:**\n",
        "\n",
        "Trains models sequentially, where each new model focuses on correcting errors of the previous one.\n",
        "\n",
        "Focuses on reducing bias (and also variance).\n",
        "\n",
        "Example: AdaBoost, XGBoost."
      ],
      "metadata": {
        "id": "FTzugY0P8kUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:**Bootstrap Sampling:**\n",
        "It is a statistical method where we create new datasets by randomly sampling (with replacement) from the original dataset, so some samples may appear multiple times while others may be left out.\n",
        "\n",
        "Role in Bagging (e.g., Random Forest):\n",
        "\n",
        "Each model (tree) is trained on a different bootstrap sample of the data.\n",
        "\n",
        "This creates diversity among the models, reducing overfitting and variance.\n",
        "\n",
        "Final prediction is made by averaging (regression) or voting (classification)."
      ],
      "metadata": {
        "id": "k6zbrUiq9WAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer:**Out-of-Bag (OOB) Samples:**\n",
        "In bootstrap sampling (used in Bagging/Random Forest), since each new dataset is created by sampling with replacement, about one-third of the original data is left out in each bootstrap sample. These unused data points are called OOB samples.\n",
        "\n",
        "OOB Score:\n",
        "\n",
        "OOB samples act like a built-in validation set.\n",
        "\n",
        "After training each tree, its OOB samples (not seen during training) are used to test it.\n",
        "\n",
        "The combined accuracy/error across all trees on their OOB samples is called the OOB score."
      ],
      "metadata": {
        "id": "cX10TIas9glw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest\n",
        "\n",
        "Answer:Feature Importance: Decision Tree vs. Random Forest\n",
        "\n",
        "Single Decision Tree:\n",
        "\n",
        "Feature importance is based on how much a feature reduces impurity (e.g., Gini, entropy, variance) at its splits.\n",
        "\n",
        "Importance can be biased toward features that allow more splits or have many categories.\n",
        "\n",
        "Since it’s only one tree, results may be unstable (high variance).\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "Aggregates feature importance across many trees, making it more reliable and stable.\n",
        "\n",
        "Reduces bias and variance compared to a single tree.\n",
        "\n",
        "Two common ways:\n",
        "\n",
        "Mean Decrease in Impurity (MDI): average impurity reduction across trees.\n",
        "\n",
        "Mean Decrease in Accuracy (MDA): measures drop in accuracy when a feature is randomly shuffled."
      ],
      "metadata": {
        "id": "txoTvdKK9yax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scoresQuestion 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores\n",
        "\n"
      ],
      "metadata": {
        "id": "TIHQhYjZ-A_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6hOAtEW76e3",
        "outputId": "bcdca151-5184-4585-f79b-ed59c4360230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                  Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance and print top 5\n",
        "top5 = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\\n\", top5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "XOxZzQm5-UlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(), # Changed base_estimator to estimator\n",
        "    n_estimators=50,      # number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of Single Decision Tree:\", acc_dt)\n",
        "print(\"Accuracy of Bagging Classifier :\", acc_bag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPDN543a-Z55",
        "outputId": "58080b35-1ad4-40c4-eb3a-f0a00c7b46f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "SSXb-38l-uAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy on Test Set:\", final_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvl5iCRT-xRd",
        "outputId": "3b2573e3-9511-4e88-c4ba-5ddca7cfde64"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Accuracy on Test Set: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "tek9Lxxh-6PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor with Decision Trees\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print comparison\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bag)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA5_jEpm_C4o",
        "outputId": "a6337f2f-dfbb-43d7-f319-13633b421f22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer:\n",
        "\n",
        " **Step 1:** **Choose between Bagging or Boosting**\n",
        "\n",
        "Bagging (e.g., Random Forest) → Best when the main problem is high variance (unstable predictions). It reduces overfitting by averaging many independent models.\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM) → Best when the main problem is high bias (model underfits). It builds models sequentially, focusing on hard-to-predict cases.\n",
        "For loan default prediction (imbalanced + complex patterns), Boosting is often preferred since it captures non-linear interactions and rare patterns better.\n",
        "\n",
        "**Step 2: Handle Overfitting**\n",
        "\n",
        "Use techniques like:\n",
        "\n",
        "Regularization in boosting (shrinkage/learning rate, max_depth).\n",
        "\n",
        "Early stopping (stop training when validation performance stops improving).\n",
        "\n",
        "Cross-validation to tune hyperparameters.\n",
        "\n",
        "Feature selection/engineering to avoid noise.\n",
        "\n",
        "**Step 3: Select Base Models**\n",
        "\n",
        "Base models depend on data complexity:\n",
        "\n",
        "Decision Trees (most common in ensemble methods).\n",
        "\n",
        "Logistic Regression / Linear Models (if data is highly linear).\n",
        "\n",
        "Neural Nets / Gradient Boosted Trees (for very large datasets with non-linear patterns).\n",
        "In practice, Decision Trees are the best base learners for bagging and boosting in tabular financial data.\n",
        "\n",
        "**Step 4: Evaluate Performance using Cross-Validation**\n",
        "\n",
        "Apply Stratified k-Fold Cross-Validation (to maintain class balance since defaults are rare).\n",
        "\n",
        "Metrics to use:\n",
        "\n",
        "AUC-ROC (to evaluate ability to separate defaulters vs non-defaulters).\n",
        "\n",
        "Precision, Recall, F1 (important because catching defaulters is more critical than overall accuracy).\n",
        "\n",
        "Confusion Matrix to understand business trade-offs (false positives vs false negatives).\n",
        "\n",
        "**Step 5: Justify How Ensemble Learning Improves Decision-Making**\n",
        "\n",
        "Reduces risk: By combining multiple models, predictions are more robust and less sensitive to noise.\n",
        "\n",
        "Improves accuracy: Boosting captures complex, non-linear relations in customer behavior.\n",
        "\n",
        "Better generalization: Bagging prevents overfitting by averaging across models.\n",
        "\n",
        "Business impact: More reliable default predictions → lower financial losses, better credit risk management, and improved trust in the institution’s lending system."
      ],
      "metadata": {
        "id": "nmZkcWpZ_R4N"
      }
    }
  ]
}