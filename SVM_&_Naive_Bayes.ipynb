{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Ans: A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "\n",
        "It works by finding the best hyperplane that separates data points of different classes with the maximum margin (the distance between the hyperplane and the nearest data points, called support vectors).\n",
        "\n",
        "For linearly separable data, SVM draws a straight line (in 2D) or a plane (in higher dimensions).\n",
        "\n",
        "For non-linear data, it uses the kernel trick (e.g., polynomial, RBF) to map data into higher dimensions where it becomes separable"
      ],
      "metadata": {
        "id": "j5RffSl3cv2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Ans: Hard Margin SVM:\n",
        "\n",
        "Assumes data is perfectly linearly separable.\n",
        "\n",
        "Finds a hyperplane that separates classes with no misclassification allowed.\n",
        "\n",
        "Works well only when there is no noise or overlap in data.\n",
        "\n",
        "Soft Margin SVM:\n",
        "\n",
        "Allows some misclassifications by introducing a penalty term.\n",
        "\n",
        "Balances between maximizing margin and minimizing classification errors.\n",
        "\n",
        "Controlled by a parameter C (high C → less tolerance to errors, low C → wider margin with more tolerance).\n"
      ],
      "metadata": {
        "id": "ldH5BaOZdQNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "Ans: The Kernel Trick in SVM is a method that allows the algorithm to handle non-linearly separable data by mapping it into a higher-dimensional space without explicitly computing the transformation. This makes it possible to find a separating hyperplane in complex datasets.\n",
        "\n",
        "Use case: Commonly used when data is not linearly separable. For instance, in image classification, RBF can separate data points that form circular or irregular boundaries."
      ],
      "metadata": {
        "id": "Avu5RcC9dX55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "Ans: A Naïve Bayes Classifier is a supervised machine learning algorithm based on Bayes’ Theorem, used mainly for classification tasks such as spam filtering, sentiment analysis, and text categorization.\n",
        "\n",
        "It is called “naïve” because it makes a strong assumption that all features are independent of each other, which is rarely true in real-world data. Despite this simplification, it often works surprisingly well, especially for high-dimensional datasets like text."
      ],
      "metadata": {
        "id": "hwx3AcbndvKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Ans: 1. Gaussian Naïve Bayes\n",
        "\n",
        "Assumes features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Suitable for continuous data (e.g., height, weight, exam scores).\n",
        "\n",
        "Example: Classifying patients based on continuous medical measurements (blood pressure, cholesterol).\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "Assumes features are counts or frequencies.\n",
        "\n",
        "Suitable for discrete data (word counts in text, event occurrences).\n",
        "\n",
        "Example: Text classification or spam detection, where features are word frequencies.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Assumes features are binary (0/1).\n",
        "\n",
        "Suitable when data represents presence/absence of a feature.\n",
        "\n",
        "Example: Document classification where a word is either present (1) or absent (0)"
      ],
      "metadata": {
        "id": "uTXbtZ4debaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cX7UWmiAev38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Support vectors\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRwg4H5YcyQ8",
        "outputId": "91b5f044-8d5f-4a03-d7f0-c9f46d2f5e05"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "2nxL0S7OfX9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLUmCMywbdtS",
        "outputId": "b748dfe7-c8a8-4c7c-9236-f33dcaf5a776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "jM1M-sVpfif3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# GridSearchCV with cross-validation\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Accuracy on Test Data:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3F8gJS3fjjm",
        "outputId": "a87a941c-a4cf-4e1e-a83e-ac1f43e633b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on Test Data: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "U-mVfsYhfv_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a subset of the 20 newsgroups dataset (for binary classification)\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Naïve Bayes Classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIOQxhW2fxyA",
        "outputId": "e716c10e-391b-44f0-98db-ad77ca44543b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Ans: Approach Explanation\n",
        "\n",
        "Preprocessing the Data\n",
        "\n",
        "Handle missing data: Replace missing text with an empty string or drop rows if excessive.\n",
        "\n",
        "Text vectorization: Use TfidfVectorizer to convert emails into numerical features while reducing the effect of frequent words.\n",
        "\n",
        "Normalization: Optional, depending on model.\n",
        "\n",
        "Choosing the Model\n",
        "\n",
        "Naïve Bayes: Fast, works well for text (esp. with word counts).\n",
        "\n",
        "SVM: More accurate but slower on large text data.\n",
        "\n",
        "👉 For large-scale email classification, Multinomial Naïve Bayes is a good first choice due to its efficiency and performance with text data.\n",
        "\n",
        "Handling Class Imbalance\n",
        "\n",
        "Use class weights (for SVM) or resampling techniques (SMOTE/undersampling).\n",
        "\n",
        "Alternatively, tune the decision threshold to improve recall for the minority class (spam).\n",
        "\n",
        "Evaluation Metrics\n",
        "\n",
        "Accuracy alone is misleading with imbalance.\n",
        "\n",
        "Use Precision, Recall, F1-score, and ROC-AUC.\n",
        "\n",
        "High recall for spam is important to catch unwanted emails.\n",
        "\n",
        "Business Impact\n",
        "\n",
        "Reduces spam exposure, improving productivity and security.\n",
        "\n",
        "Saves employees’ time, prevents phishing/malware attacks.\n",
        "\n",
        "Builds trust with customers by ensuring legitimate communications aren’t marked as spam."
      ],
      "metadata": {
        "id": "_GgTZwgEgDSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import numpy as np\n",
        "\n",
        "# Load a text dataset (simulate spam detection: spam categories vs ham categories)\n",
        "categories = ['rec.autos', 'talk.politics.misc']  # simulating ham vs spam\n",
        "data = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Handle missing values: replace None with empty string\n",
        "texts = [doc if doc is not None else \"\" for doc in data.data]\n",
        "y = data.target\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Handle class imbalance with oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "y_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SirfuKKPgEdk",
        "outputId": "e7e1d657-7813-4b2c-ec9e-923960049694"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "         rec.autos       1.00      0.99      0.99       289\n",
            "talk.politics.misc       0.99      1.00      0.99       305\n",
            "\n",
            "          accuracy                           0.99       594\n",
            "         macro avg       0.99      0.99      0.99       594\n",
            "      weighted avg       0.99      0.99      0.99       594\n",
            "\n",
            "ROC-AUC Score: 0.9997050314822168\n"
          ]
        }
      ]
    }
  ]
}